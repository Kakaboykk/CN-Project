import os
import time
import random
import pandas as pd
import numpy as np
import matplotlib
# Try to use interactive backend, fallback to saving if needed
try:
    matplotlib.use('TkAgg')  # Try TkAgg for Windows
    import matplotlib.pyplot as plt
    plt.ion()  # Turn on interactive mode
except:
    import matplotlib.pyplot as plt
    print("Note: Using non-interactive backend. Plots will be saved to files.")

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn

from ag_utils import Corpus
from ag_utils import parse_ag_file
from ag_utils import parse_node_properties

from synthetic_data import gene_dataset

from models import NN, GCN, GAT, GCN_EW
from model_utils import train, predict_prob, evaluate_performance

# parse attack graph file generated by MulVAL tool
attack_graph_path = '../mulval_attack_graph/AttackGraph.dot'
nodes, edges, node_properties = parse_ag_file(attack_graph_path)
node_dict = parse_node_properties(nodes, node_properties)

# save node label into corpus object
corpus = Corpus(node_dict)
num_tokens = corpus.get_num_tokens()
node_features = corpus.get_node_features()
node_types = corpus.get_node_types()
vocab_size = len(corpus.dictionary)
print('\n' + '='*70)
print('ATTACK GRAPH PROCESSING')
print('='*70)
print(f'Vocabulary size: {vocab_size}')
print(f'Number of tokens: {num_tokens}')
print(f'Node features shape: {node_features.shape}')

# statistics of the encoded attack graph
num_nodes = len(nodes)
num_node_features = node_features.shape[1]
num_edges = len(edges)

action_nodes = corpus.get_action_nodes()
action_node_idx = list(action_nodes.keys())
num_action_nodes = len(action_node_idx)

print(f'\nNumber of nodes: {num_nodes}')
print(f'Number of node features: {num_node_features}')
print(f'Number of edges: {num_edges}')
print(f'Action node indices: {action_node_idx}')
print(f'Number of action nodes: {num_action_nodes}')
print('='*70)

# var 'action_mask' is for representing the attack scenarios in attack graph (i.e., the privilege nodes)
action_mask = action_node_idx

# adj matrix and edge index
adj_matrix = torch.zeros(len(nodes), len(nodes))

for edge in edges:
    source_node, target_node = edge
    source_index = nodes.index(source_node)
    target_index = nodes.index(target_node)
    adj_matrix[source_index][target_index] = 1

edge_index = adj_matrix.nonzero().t().contiguous()

assert edge_index.shape[0]==2

# prepare Dataset 1
num_benign = 1500
num_malic = 500
rt_meas_dim = 78
sample_method = 'synthetic'

X, Y = gene_dataset(num_benign, num_malic, num_nodes, action_nodes, rt_meas_dim)
num_samples = X.shape[0]

# rescale the data with min-max scaler
scaler = MinMaxScaler(feature_range=(0, 1))
X = scaler.fit_transform(X.view(-1, rt_meas_dim))
X = torch.from_numpy(X).float().view(num_samples, num_nodes, rt_meas_dim)

# associate node features in attack graph with real-time measurements
node_feat_ts = torch.stack([node_features for _ in range(len(X))], dim=0)
X = torch.cat((node_feat_ts, X), dim=2)

# split the data into training, validation and test set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42, stratify=Y_train)
print('\n' + '='*70)
print('DATASET LOADING')
print('='*70)
print(f'Dataset shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')
print(f'Label shapes - Train: {Y_train.shape}, Val: {Y_val.shape}, Test: {Y_test.shape}')

# load Dataset 1 used in the paper
data_path = '../datasets/synt/'
X_train = torch.load(data_path+'X_train-{}.pth'.format(sample_method))
X_val   = torch.load(data_path+'X_val-{}.pth'.format(sample_method))
X_test  = torch.load(data_path+'X_test-{}.pth'.format(sample_method))
Y_train = torch.load(data_path+'Y_train-{}.pth'.format(sample_method))
Y_val   = torch.load(data_path+'Y_val-{}.pth'.format(sample_method))
Y_test  = torch.load(data_path+'Y_test-{}.pth'.format(sample_method))
print(f'Loaded preprocessed dataset shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')
print('='*70)

# hyperparameters for training
in_dim = X_train.shape[-1]
hidden_dim = 20
out_dim = 1
lr = 0.001
device = 'cpu'

print('\n' + '='*70)
print('MODEL TRAINING')
print('='*70)

# model initialization
models = {}
model_NN = NN(rt_meas_dim, hidden_dim, out_dim)
model_GCN = GCN(in_dim, hidden_dim, out_dim)
model_GCN_EW = GCN_EW(in_dim, hidden_dim, out_dim, edge_index)
model_GAT = GAT(hidden_channels=hidden_dim, heads=4, in_dim=in_dim, out_dim=out_dim)

models['NN'] = model_NN
models['GCN'] = model_GCN
models['GCN-EW'] = model_GCN_EW
models['GAT'] = model_GAT

for name, model in models.items():
    model.name = name
    model.action_mask = action_mask
    num_epochs = 2 # early stop when overfitting observed

    print(f'{model.name} start training...')
    time_start = time.time()
    print('model: ', model) 
    train(model, lr, num_epochs, X_train, Y_train, X_val, Y_val, edge_index, rt_meas_dim, device)       
    time_end = time.time()
    print('time cost: ', time_end - time_start)  
    print(f'{model.name} training finished!')

    print(f'\n{model.name} Results:')
    print(f'  Training Accuracy: {model.stat["acc_train"][-1]:.4f}')
    print(f'  Validation Accuracy: {model.stat["acc_val"][-1]:.4f}')
    print(f'  Training Loss: {model.stat["loss_train"][-1]:.4f}')
    print(f'  Validation Loss: {model.stat["loss_val"][-1]:.4f}')
    print('-'*70)

print('\n' + '='*70)
print('PERFORMANCE EVALUATION')
print('='*70)

# plot the roc curve
from sklearn.metrics import roc_curve, auc
fig, ax = plt.subplots(figsize=(5, 5))
for name, model in models.items():
    prob = predict_prob(model, X_test, edge_index)
    y_probs = prob.view(-1, 2)

    fpr, tpr, thresholds = roc_curve(Y_test.view(-1), y_probs[:, 1])
    roc_auc = auc(fpr, tpr)

    ax.plot(fpr, tpr, label='{} (AUC = {:.4f})'.format(model.name, roc_auc))
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_title('Receiver Operating Characteristic (ROC) Curve')
    ax.legend(loc="lower right")
ax.grid()
print('\n' + '='*70)
print('ROC Curve Plot - Displaying...')
print('='*70)
plt.savefig('roc_curve.png', dpi=150, bbox_inches='tight')
print('ROC curve saved to: roc_curve.png')
try:
    plt.show(block=False)
    plt.pause(0.1)  # Brief pause to allow display
except:
    print('Plot window may not be available. Check roc_curve.png file.')
# Keep plot open for a moment, then close
time.sleep(1)
plt.close()

# performance evaluation
metrics = evaluate_performance(models, X_test, Y_test, edge_index, device)
df = pd.DataFrame(metrics)
print('\n' + '='*70)
print('PERFORMANCE METRICS - TEST SET')
print('='*70)
# Display with better formatting
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_rows', None)
print(df.to_string(index=False))
print('\nDetailed Metrics Breakdown:')
print('-'*70)
for idx, row in df.iterrows():
    print(f"\n{row['model']} Model:")
    print(f"  True Positives (TP): {row['TP']}")
    print(f"  True Negatives (TN): {row['TN']}")
    print(f"  False Positives (FP): {row['FP']}")
    print(f"  False Negatives (FN): {row['FN']}")
    print(f"  Accuracy: {row['accuracy']}")
    print(f"  Precision: {row['precision']}")
    print(f"  Recall: {row['recall']}")
    print(f"  F1-Score: {row['f1']}")
    print(f"  AUC: {row['auc']}")
    print(f"  False Positive Rate: {row['fpr']}")
    print(f"  False Negative Rate: {row['fnr']}")
print('='*70)

# plot the TP, FP, TN, FN
fig, axs = plt.subplots(2, 2, figsize=(5, 5))
axs = axs.ravel()

bar_width = 0.5
labels = ['TN', 'FP', 'FN', 'TP']

for i, label in enumerate(labels):
    for j, name in enumerate(models):
        rects = axs[i].bar(j, metrics[j][label], width=bar_width, label=name)
    axs[i].set_xticks(np.arange(len(models)))
    axs[i].set_xticklabels(models.keys())
    for tick in axs[i].xaxis.get_major_ticks():
        tick.label1.set_fontsize(10)
    axs[i].set_title(label)

handles, labels = axs[0].get_legend_handles_labels()

plt.tight_layout()
print('\n' + '='*70)
print('Confusion Matrix Components Plot - Displaying...')
print('='*70)
plt.savefig('confusion_matrix_components.png', dpi=150, bbox_inches='tight')
print('Confusion matrix components saved to: confusion_matrix_components.png')
try:
    plt.show(block=False)
    plt.pause(0.1)  # Brief pause to allow display
except:
    print('Plot window may not be available. Check confusion_matrix_components.png file.')
# Keep plot open for a moment, then close
time.sleep(1)
plt.close()

# robustness evaluation
print('\nRunning robustness evaluation...')
for i in range(Y_test.shape[0]):
    for j in range(len(action_mask)):
        if Y_test[i, j] == 1:
            for k in range(rt_meas_dim):
                if True:
                    X_test[i, action_mask[j],-rt_meas_dim+k] -= torch.normal(mean=0.0, std=0.05, size=(1,)).item()

metrics = evaluate_performance(models, X_test, Y_test, edge_index, device)
df = pd.DataFrame(metrics)
print('\n' + '='*70)
print('ROBUSTNESS EVALUATION METRICS - AFTER NOISE ADDITION')
print('='*70)
print('(Gaussian noise with std=0.05 added to compromised nodes)')
print(df.to_string(index=False))
print('\nRobustness Analysis:')
print('-'*70)
for idx, row in df.iterrows():
    model_name = row['model']
    acc = float(row['accuracy'])
    print(f"{model_name}: Accuracy = {acc:.4f}")
print('='*70)

print('\n' + '='*70)
print('SCRIPT EXECUTION COMPLETED SUCCESSFULLY!')
print('='*70)
print('\nFinal Summary:')
print('-'*70)
print(f'✓ Trained {len(models)} models: {", ".join(models.keys())}')
best_model = max(models.keys(), key=lambda m: models[m].stat["acc_val"][-1])
best_acc = models[best_model].stat["acc_val"][-1]
print(f'✓ Best validation accuracy: {best_model} with {best_acc:.4f}')
print(f'✓ Performance metrics calculated and displayed above')
print(f'✓ ROC curve: Saved to roc_curve.png')
print(f'✓ Confusion matrix: Saved to confusion_matrix_components.png')
print(f'✓ Robustness evaluation completed')
print('='*70)

